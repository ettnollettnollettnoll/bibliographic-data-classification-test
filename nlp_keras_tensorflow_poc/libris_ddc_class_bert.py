# -*- coding: utf-8 -*-
"""libris_ddc_class_bert.ipynb

Automatically generated by Colab.

# Classification of bibliographic data from LIBRIS using DDC
  
  
The data used for training and testing is extracted from bibliographic MARC records https://www.loc.gov/marc/bibliographic/ downloaded from the Swedish union catalogue LIBRIS https://libris.kb.se/  
  
For classification the Dewey Decimal Classification (DDC) is used. https://www.oclc.org/en/dewey/resources.html Only the ten main classes are used.

Start with some imports.
"""

!pip install transformers

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from transformers import AutoTokenizer
from transformers import TFBertModel
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix, precision_score, recall_score, classification_report
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, LSTM, Conv2D, Conv1D, MaxPooling1D, Dense, Dropout, GlobalMaxPooling1D, Input, Bidirectional, concatenate, Flatten, GlobalAveragePooling1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard
from tensorflow.keras.utils import plot_model

"""# Download and process the raw data
  
The data vas originally downloaded as MARC XML from LIBRIS in batches via the Xsearch API https://libris.kb.se/help/xsearch_eng.jsp?redirected=true&language=en
"""

#Download the data previously collected from LIBRIS
data_original = pd.read_csv('https://maxschreck.se/nlp_2023/libris_records_extended.csv', dtype=str, na_values='', on_bad_lines='skip', sep=',')
#Print some information
data_original.info()
data_original.shape

"""Remove any possible faulty records that are missing title or DDC code. Then also remove low quality records since only a subset of the records will be used for training."""

#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html
#Remove missing values, axis=0, or ‘index’ : Drop rows which contain missing values.
#Remove rows which are missing title or ddc
data_original['title'] = data_original['title'].replace('', np.nan)
data_original['ddc'] = data_original['ddc'].replace('', np.nan)
data_original = data_original.dropna(axis=0, subset=['title'])
data_original = data_original.dropna(axis=0, subset=['ddc'])

# drop rows without sao subject headings
data_original['subject_headings_sao'] = data_original['subject_headings_sao'].replace('', np.nan)
data_original = data_original.dropna(axis=0, subset=['subject_headings_sao'])

"""Truncate the DDC code to only one character since classification will only be performed at the first level. Then investigate the distribution over classes."""

#Truncate the ddc to one character
data_original['ddc'] = data_original['ddc'].str[0:1]

data_original['ddc'].value_counts()

"""From above some some faulty classes (non-integer) can be identified, remove these records."""

#Remove records with faulty labels
data_original['ddc'] = data_original['ddc'].str.replace('[^0-9]','', regex=True)
data_original['ddc'] = data_original['ddc'].replace('', np.nan)
data_original = data_original.dropna(axis=0, subset=['ddc'])
#Transform the ddc values to integers
data_original['ddc'] = data_original['ddc'].map(int)
data_original['ddc'].value_counts()

"""A quite big class imbalance can be observed. In order to reduce the imbalance and to make the training faster use XXXX samples from each class."""

#Select 10 000 random records from each class

data_reduced = data_original.groupby('ddc').apply(pd.DataFrame.sample, n=10000).reset_index(drop=True)
data_reduced['ddc'].value_counts()

"""# Split the data into training, valiadtion and test
  
A wrapper function to train_test_split() from sklearn is used for creating the sets.
  
Then extract the fields that will be used for features.
"""

#Function provided by stackoverflowuser2010
#it is basically a wrapper that calls train_test_split twice

def split_stratified_into_train_val_test(df_input, stratify_colname='y',
                                         frac_train=0.6, frac_val=0.15, frac_test=0.25,
                                         random_state=None):
    '''
    Splits a Pandas dataframe into three subsets (train, val, and test)
    following fractional ratios provided by the user, where each subset is
    stratified by the values in a specific column (that is, each subset has
    the same relative frequency of the values in the column). It performs this
    splitting by running train_test_split() twice.

    Parameters
    ----------
    df_input : Pandas dataframe
        Input dataframe to be split.
    stratify_colname : str
        The name of the column that will be used for stratification. Usually
        this column would be for the label.
    frac_train : float
    frac_val   : float
    frac_test  : float
        The ratios with which the dataframe will be split into train, val, and
        test data. The values should be expressed as float fractions and should
        sum to 1.0.
    random_state : int, None, or RandomStateInstance
        Value to be passed to train_test_split().

    Returns
    -------
    df_train, df_val, df_test :
        Dataframes containing the three splits.
    '''

    if frac_train + frac_val + frac_test != 1.0:
        raise ValueError('fractions %f, %f, %f do not add up to 1.0' % \
                         (frac_train, frac_val, frac_test))

    if stratify_colname not in df_input.columns:
        raise ValueError('%s is not a column in the dataframe' % (stratify_colname))

    X = df_input # Contains all columns.
    y = df_input[[stratify_colname]] # Dataframe of just the column on which to stratify.

    # Split original dataframe into train and temp dataframes.
    df_train, df_temp, y_train, y_temp = train_test_split(X,
                                                          y,
                                                          stratify=y,
                                                          test_size=(1.0 - frac_train),
                                                          random_state=random_state)

    # Split the temp dataframe into val and test dataframes.
    relative_frac_test = frac_test / (frac_val + frac_test)
    df_val, df_test, y_val, y_test = train_test_split(df_temp,
                                                      y_temp,
                                                      stratify=y_temp,
                                                      test_size=relative_frac_test,
                                                      random_state=random_state)

    assert len(df_input) == len(df_train) + len(df_val) + len(df_test)

    return df_train, df_val, df_test

data_reduced['contents'] = data_reduced['contents'].fillna('')
data_reduced['summary'] = data_reduced['summary'].fillna('')

data_train, data_val, data_test = split_stratified_into_train_val_test(data_reduced, stratify_colname='ddc', random_state=42)

X_train = data_train['title'] + " " + data_train['subject_headings_all'] + " " + data_train['contents'] + " " + data_train['summary']
y_train = data_train['ddc']

X_val = data_val['title'] + " " + data_val['subject_headings_all'] + " " + data_val['contents'] + " " + data_val['summary']
y_val = data_val['ddc']

X_test = data_test['title'] + " " + data_test['subject_headings_all'] + " " + data_test['contents'] + " " + data_test['summary']
y_test = data_test['ddc']

maxlen = 256

#maxlen = X_train.map(lambda x: len(x.split())).max()

"""# Tokenize the data"""

#Use the basic Swedish BERT model from Kungliga Biblioteket
tokenizer = AutoTokenizer.from_pretrained("KB/bert-base-swedish-cased", add_special_tokens=True, max_length=int(maxlen), pad_to_max_length=True)



def tokenize(sentences, tokenizer):
    input_ids, input_masks, input_segments = [],[],[]
    for sentence in tqdm(sentences):
        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=maxlen, pad_to_max_length=True,
                                             return_attention_mask=True, return_token_type_ids=True)
        input_ids.append(inputs['input_ids'])
        input_masks.append(inputs['attention_mask'])
        input_segments.append(inputs['token_type_ids'])

    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32')

# Tokenize desc and title train data
X_train = tokenize(X_train, tokenizer)
X_val = tokenize(X_val, tokenizer)
X_test = tokenize(X_test, tokenizer)

"""# Create the model
  
In this case a BERT model is used.
"""

transformer_model = TFBertModel.from_pretrained('KB/bert-base-swedish-cased')

input_ids_in = tf.keras.layers.Input(shape=(maxlen,), name='input_token', dtype='int32')
input_masks_in = tf.keras.layers.Input(shape=(maxlen,), name='masked_token', dtype='int32')

embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]
X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(embedding_layer)
X = tf.keras.layers.GlobalMaxPool1D()(X)
X = tf.keras.layers.Dense(64, activation='relu')(X)
X = tf.keras.layers.Dropout(0.2)(X)
X = tf.keras.layers.Dense(10, activation='softmax')(X)
model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)

for layer in model.layers[:3]:
    layer.trainable = False

model.summary()

plot_model(model, "model.png", show_shapes=True)

"""# Compile and fit the model
  
Some callbacks are also defined. ModelCheckpoit is used to save the weights of the best model (going by validation accuracy) and using that for evaluation/testing. (Since the best overall weights will be used EarlyStopping is not used). TensorBoard is also used for logging the training.
"""

callbacks = [
#     EarlyStopping(
#         monitor='val_accuracy',
#         min_delta=1e-4,
#         patience=4,
#         verbose=1
#     ),
    ModelCheckpoint(
        filepath='weights.h5',
        monitor='val_accuracy',
        mode='max',
        save_best_only=True,
        save_weights_only=True,
        verbose=1
    ),
    TensorBoard(
        "/tmp/tb_logs",
        histogram_freq=1,
        write_graph=True,
        write_images=True,
        update_freq='epoch',
        profile_batch=2,
        embeddings_freq=1
    )
]

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, batch_size=512, validation_data=(X_val, y_val), epochs=20, callbacks=callbacks)

# Commented out IPython magic to ensure Python compatibility.
#If running in Colab, the following two commands will show you the TensorBoard inside Colab.
# %load_ext tensorboard
# %tensorboard --logdir /tmp/tb_logs

#Load the best weights
model.load_weights('weights.h5')

"""# Evaluate the model
  
  Use the test set to evaluate the model.
  Print precision, recall, f1-score, accuracy and a confusion matrix showing the total result and the result over classes.
"""

preds = [np.argmax(i) for i in model.predict(X_test)]

print(classification_report(y_test, preds))

#Print comfusion matrix
labels = ['Computer science, information\nand general works',
          'Philosophy and psychology',
          'Religion',
          'Social sciences',
          'Language',
          'Pure science',
          'Technology',
          'Arts and recreation',
          'Literature',
          'History and geography']
cm  = confusion_matrix(y_test, preds)
plt.figure()
plot_confusion_matrix(cm, figsize=(16,12), hide_ticks=True, cmap=plt.cm.Blues)
plt.xticks(range(10), labels, fontsize=12, rotation='vertical')
plt.yticks(range(10), labels, fontsize=12)
plt.show()