# -*- coding: utf-8 -*-
"""libris_ddc_class_non-bert.ipynb

Automatically generated by Colab.

# Classification of bibliographic data from LIBRIS using DDC
  
  
The data used for training and testing is extracted from bibliographic MARC records https://www.loc.gov/marc/bibliographic/ downloaded from the Swedish union catalogue LIBRIS https://libris.kb.se/  
  
For classification the Dewey Decimal Classification (DDC) is used. https://www.oclc.org/en/dewey/resources.html Only the ten main classes are used.

Start with some imports.
"""

!pip install transformers

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix, precision_score, recall_score, classification_report
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, LSTM, Conv2D, Conv1D, MaxPooling1D, Dense, Dropout, GlobalMaxPooling1D, Input, Bidirectional, concatenate, Flatten, GlobalAveragePooling1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard
from tensorflow.keras.utils import plot_model

"""# Download and process the raw data
  
The data vas originally downloaded as MARC XML from LIBRIS in batches via the Xsearch API https://libris.kb.se/help/xsearch_eng.jsp?redirected=true&language=en
"""

#Download the data previously collected from LIBRIS
data_original = pd.read_csv('https://maxschreck.se/nlp_2023/libris_records.csv', dtype=str, na_values='')
#Print some information
data_original.info()
data_original.shape

"""Remove any possible faulty records that are missing title or DDC code. Then also remove low quality records since only a subset of the records will be used for training."""

#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html
#Remove missing values, axis=0, or ‘index’ : Drop rows which contain missing values.
#Remove rows which are missing title or ddc
data_original['title'] = data_original['title'].replace('', np.nan)
data_original['ddc'] = data_original['ddc'].replace('', np.nan)
data_original = data_original.dropna(axis=0, subset=['title'])
data_original = data_original.dropna(axis=0, subset=['ddc'])

# drop rows without sao subject headings
data_original['subject_headings_sao'] = data_original['subject_headings_sao'].replace('', np.nan)
data_original = data_original.dropna(axis=0, subset=['subject_headings_sao'])

"""Truncate the DDC code to only one character since classification will only be performed at the first level. Then investigate the distribution over classes."""

#Truncate the ddc to one character
data_original['ddc'] = data_original['ddc'].str[0:1]

data_original['ddc'].value_counts()

"""From above some some faulty classes (non-integer) can be identified, remove these records."""

#Remove records with faulty labels
data_original['ddc'] = data_original['ddc'].str.replace('[^0-9]','', regex=True)
data_original['ddc'] = data_original['ddc'].replace('', np.nan)
data_original = data_original.dropna(axis=0, subset=['ddc'])
#Transform the ddc values to integers
data_original['ddc'] = data_original['ddc'].map(int)
data_original['ddc'].value_counts()

"""A quite big class imbalance can be observed. In order to reduce the imbalance and to make the training faster use XXXX samples from each class."""

#Select 10 000 random records from each class
data_reduced = data_original.groupby('ddc').apply(pd.DataFrame.sample, n=10000).reset_index(drop=True)
data_reduced['ddc'].value_counts()

"""# Split the data into training, valiadtion and test
  
A wrapper function to train_test_split() from sklearn is used for creating the sets.
  
Then extract the fields that will be used for features.
"""

#Function provided by stackoverflowuser2010
#it is basically a wrapper that calls train_test_split twice

def split_stratified_into_train_val_test(df_input, stratify_colname='y',
                                         frac_train=0.6, frac_val=0.15, frac_test=0.25,
                                         random_state=None):
    '''
    Splits a Pandas dataframe into three subsets (train, val, and test)
    following fractional ratios provided by the user, where each subset is
    stratified by the values in a specific column (that is, each subset has
    the same relative frequency of the values in the column). It performs this
    splitting by running train_test_split() twice.

    Parameters
    ----------
    df_input : Pandas dataframe
        Input dataframe to be split.
    stratify_colname : str
        The name of the column that will be used for stratification. Usually
        this column would be for the label.
    frac_train : float
    frac_val   : float
    frac_test  : float
        The ratios with which the dataframe will be split into train, val, and
        test data. The values should be expressed as float fractions and should
        sum to 1.0.
    random_state : int, None, or RandomStateInstance
        Value to be passed to train_test_split().

    Returns
    -------
    df_train, df_val, df_test :
        Dataframes containing the three splits.
    '''

    if frac_train + frac_val + frac_test != 1.0:
        raise ValueError('fractions %f, %f, %f do not add up to 1.0' % \
                         (frac_train, frac_val, frac_test))

    if stratify_colname not in df_input.columns:
        raise ValueError('%s is not a column in the dataframe' % (stratify_colname))

    X = df_input # Contains all columns.
    y = df_input[[stratify_colname]] # Dataframe of just the column on which to stratify.

    # Split original dataframe into train and temp dataframes.
    df_train, df_temp, y_train, y_temp = train_test_split(X,
                                                          y,
                                                          stratify=y,
                                                          test_size=(1.0 - frac_train),
                                                          random_state=random_state)

    # Split the temp dataframe into val and test dataframes.
    relative_frac_test = frac_test / (frac_val + frac_test)
    df_val, df_test, y_val, y_test = train_test_split(df_temp,
                                                      y_temp,
                                                      stratify=y_temp,
                                                      test_size=relative_frac_test,
                                                      random_state=random_state)

    assert len(df_input) == len(df_train) + len(df_val) + len(df_test)

    return df_train, df_val, df_test

data_train, data_val, data_test = split_stratified_into_train_val_test(data_reduced, stratify_colname='ddc', random_state=42)

#data_train['ddc'].value_counts()

#data_val['ddc'].value_counts()

#data_test['ddc'].value_counts()

X_train = data_train['title'] + " " + data_train['subject_headings_all']
y_train = data_train['ddc']

X_val = data_val['title'] + " " + data_val['subject_headings_all']
y_val = data_val['ddc']

X_test = data_test['title'] + " " + data_test['subject_headings_all']
y_test = data_test['ddc']

maxlen = 256

#maxlen = X_train.map(lambda x: len(x.split())).max()

"""# Tokenize the data  

The tokenizer below is basically the same one that was used in assignment 2.
"""

vocabulary = {}
vocabulary["<sos>"] = 1 #start of sequence
vocabulary["<eos>"] = 2 #end of sequence

def tokenize_non_bert(sentenceData, maxlen, vocabulary):
  #create empty lists and dictionary
  sentences = []
  #loop over the reviews
  for data in sentenceData:
      #make the sentence text lower case
      data = data.lower()
      #split the sentences text into tokens (on whitespace)
      tokens = data.split()
      #create an empty list
      cleanedSentence = []
      #loop over tokens
      for token in tokens:
          #if the token does not already exist in vocabulary
          if vocabulary.get(token) is None:
              #add it to the vocabulary with the value of the number of current values + 1
              vocabulary[token] = len(vocabulary)+1
          #add the integer value of the token from the vocabulary to the cleanedSentence list
          cleanedSentence.append(vocabulary.get(token))
      #if the length of cleanedSentence is more than 1 when every token in the review has been processed
      if len(cleanedSentence) > 1:
          #if the number of tokens exceeds the defined maximum length of the sentences
          #slice it to the maximum length
          if len(cleanedSentence) > maxlen:
              cleanedSentence = cleanedSentence[:maxlen]
          #create a list of zeros the length of maximum length defined
          pad = [0]*maxlen
          #replace part of the zeros (the end part of the list) with the tokens from the sentence
          pad[-len(cleanedSentence):] = cleanedSentence
          #add the (truncated and padded) list of tokens for a sentence to the sentences list
          sentences.append(pad)

  import numpy as np #import numpy
  #create an ndarray X with the dimensons of the number of sentences and the number of tokens in sentences
  X = np.array([np.array(data) for data in sentences]).reshape((len(sentences), maxlen))

  return X, vocabulary

#print(X_train)

# Tokenize desc and title train data
X_train, vocabulary = tokenize_non_bert(X_train, maxlen, vocabulary)
X_val, vocabulary = tokenize_non_bert(X_val, maxlen, vocabulary)
X_test, vocabulary = tokenize_non_bert(X_test, maxlen, vocabulary)

"""# Create the model
  
In this case a non-BERT model is used.
"""

inputs = tf.keras.layers.Input((1, ), name="input", )
embedding_layer = tf.keras.layers.Embedding(input_dim=len(vocabulary)+1, output_dim=100, input_length=maxlen)(inputs)
x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(embedding_layer)
x = tf.keras.layers.GlobalMaxPool1D()(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = tf.keras.layers.Dense(10, activation='softmax')(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)

model.summary()

plot_model(model, "model.png", show_shapes=True)

"""# Compile and fit the model
  
Some callbacks are also defined. ModelCheckpoit is used to save the weights of the best model (going by validation accuracy) and using that for evaluation/testing. (Since the best overall weights will be used EarlyStopping is not used). TensorBoard is also used for logging the training.
"""

callbacks = [
#     EarlyStopping(
#         monitor='val_accuracy',
#         min_delta=1e-4,
#         patience=4,
#         verbose=1
#     ),
    ModelCheckpoint(
        filepath='weights.h5',
        monitor='val_accuracy',
        mode='max',
        save_best_only=True,
        save_weights_only=True,
        verbose=1
    ),
    TensorBoard(
        "/tmp/tb_logs",
        histogram_freq=1,
        write_graph=True,
        write_images=True,
        update_freq='epoch',
        profile_batch=2,
        embeddings_freq=1
    )
]

#print(X_train.shape)

#print(y_train)

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, batch_size=512, validation_data=(X_val, y_val), epochs=20, callbacks=callbacks)

# Commented out IPython magic to ensure Python compatibility.
#If running in Colab, the following two commands will show you the TensorBoard inside Colab.
# %load_ext tensorboard
# %tensorboard --logdir /tmp/tb_logs

#Load the best weights
model.load_weights('weights.h5')

"""# Evaluate the model
  
  Use the test set to evaluate the model.
  Print precision, recall, f1-score, accuracy and a confusion matrix showing the total result and the result over classes.
"""

preds = [np.argmax(i) for i in model.predict(X_test, maxlen, vocabulary)]

print(classification_report(y_test, preds))

#Print comfusion matrix
labels = ['Computer science, information\nand general works',
          'Philosophy and psychology',
          'Religion',
          'Social sciences',
          'Language',
          'Pure science',
          'Technology',
          'Arts and recreation',
          'Literature',
          'History and geography']
cm  = confusion_matrix(y_test, preds)
plt.figure()
plot_confusion_matrix(cm, figsize=(16,12), hide_ticks=True, cmap=plt.cm.Blues)
plt.xticks(range(10), labels, fontsize=12, rotation='vertical')
plt.yticks(range(10), labels, fontsize=12)
plt.show()